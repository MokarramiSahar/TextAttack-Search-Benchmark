{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import notebook as tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_DEVICE = 0\n",
    "TF_DEVICE = 1\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Metric:\n",
    "    def __init__(self):\n",
    "        self._model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "        self._model.to(device=f'cuda:{PYTORCH_DEVICE}')\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "        \n",
    "    def perplexity(self, text):\n",
    "        input_ids = self._tokenizer.encode(text)\n",
    "        input_ids = input_ids[: self._tokenizer.model_max_length - 2]\n",
    "        input_ids.insert(0, self._tokenizer.bos_token_id)\n",
    "        input_ids.append(self._tokenizer.eos_token_id)\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        input_ids = input_ids.to(device=f'cuda:{PYTORCH_DEVICE}')\n",
    "        with torch.no_grad():\n",
    "            loss = self._model(input_ids, labels=input_ids)[0].item()\n",
    "    \n",
    "        perplexity = math.exp(loss)\n",
    "        return perplexity\n",
    "    \n",
    "    def calc_metric(self, orig_text, new_text):\n",
    "        orig_perplexity = self.perplexity(orig_text)\n",
    "        new_perplexity = self.perplexity(new_text)\n",
    "        return (new_perplexity - orig_perplexity) / orig_perplexity\n",
    "    \n",
    "\n",
    "class USEMetric:\n",
    "    def __init__(self):\n",
    "        tfhub_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "        with tf.device(f'/device:GPU:{TF_DEVICE}'):\n",
    "            self._model = hub.load(tfhub_url)\n",
    "\n",
    "    def encode(self, orig_text, new_text):\n",
    "        with tf.device(f'/device:GPU:{TF_DEVICE}'):\n",
    "            return self._model([orig_text, new_text]).numpy()\n",
    "    \n",
    "    def get_angular_sim(self, emb1, emb2):\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=0)(emb1, emb2)\n",
    "        return 1 - (torch.acos(cos_sim) / math.pi)\n",
    "    \n",
    "    def calc_metric(self, orig_text, new_text):\n",
    "        orig_emb, new_emb = self.encode(orig_text, new_text)\n",
    "        orig_emb = torch.tensor(orig_emb)\n",
    "        new_emb = torch.tensor(new_emb)\n",
    "        sim = self.get_angular_sim(orig_emb, new_emb).item()\n",
    "        return sim\n",
    "\n",
    "class PercentageOfWordsChanged:\n",
    "    def calc_metric(self, orig_text, new_text):\n",
    "        orig_words = np.array(orig_text.split())\n",
    "        new_words = np.array(new_text.split())\n",
    "        words_changed = (orig_words != new_words).sum()\n",
    "        return words_changed * 100 / len(orig_words)\n",
    "    \n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.use_metric = USEMetric()\n",
    "        self.gpt2_metric = GPT2Metric()\n",
    "        self.percentageOfWordsChanged = PercentageOfWordsChanged()\n",
    "        \n",
    "    def evaluate(self, csv_file, all_successful):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df[df['result_type']==\"Successful\"]\n",
    "\n",
    "        total_sim = 0\n",
    "        total_pp_diff = 0\n",
    "        word_changed_percent = 0\n",
    "        N = 0\n",
    "        for i, row in df.iterrows():\n",
    "            original_text = row[\"original_text\"].replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "            if original_text not in all_successful:\n",
    "                continue\n",
    "            perturbed_text = row[\"perturbed_text\"].replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "            sim = self.use_metric.calc_metric(original_text, perturbed_text)\n",
    "            total_sim += sim\n",
    "            pp_diff = self.gpt2_metric.calc_metric(original_text, perturbed_text)\n",
    "            total_pp_diff += pp_diff\n",
    "            word_changed_percent += self.percentageOfWordsChanged.calc_metric(original_text, perturbed_text)\n",
    "            N += 1\n",
    "\n",
    "        return total_sim / N, total_pp_diff / N, word_changed_percent / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"bert-yelp-test\", \"bert-mr-test\", \"bert-snli-test\"]\n",
    "#models = [\"lstm-yelp-test\", \"lstm-mr-test\"]\n",
    "model_dataset_names = {\n",
    "    \"bert-mr-test\": \"BERT Movie Reviews\",\n",
    "    \"bert-yelp-test\": \"BERT Yelp Polarity\",\n",
    "    \"bert-snli-test\": \"BERT SNLI\",\n",
    "    \"lstm-mr-test\": \"LSTM Movie Reviews\",\n",
    "    \"lstm-yelp-test\": \"LSTM Yelp Polarity\",\n",
    "}\n",
    "transformations = [\"word-swap-embedding\", \"word-swap-hownet\", \"word-swap-wordnet\"]\n",
    "constraint_levels = [\"strict\"]\n",
    "search_methods = [\"greedy\", \"beam4\", \"beam8\", \"greedyWIR_unk\", \"greedyWIR_delete\", \"greedyWIR_pwws\", \"greedyWIR_gradient\",  \"greedyWIR_random\", \"genetic\", \"pso\"]\n",
    "search_method_names = {\n",
    "    'greedy': 'Greedy [b=1]',\n",
    "    'beam4': 'Beam Search [b=4]',\n",
    "    'beam8': 'Beam Search [b=8]',\n",
    "    'greedyWIR_unk': 'Greedy WIR [UNK]',\n",
    "    'greedyWIR_delete': 'Greedy WIR [DEL]',\n",
    "    'greedyWIR_random': 'Greedy WIR [RAND]',\n",
    "    'greedyWIR_random': 'Greedy WIR [Gradient]',\n",
    "    'greedyWIR_pwws': 'Greedy WIR [PWWS]',\n",
    "    'genetic': 'Genetic Algorithm',\n",
    "    'pso': 'Particle Swarm Optimization'\n",
    "}\n",
    "RESULT_ROOT_DIR = \"./results\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d1a7d7d7074f888d4584db83a75f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=90.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_successful_attacks = []\n",
    "num_files = len(models) * len(transformations) * len(constraint_levels) * len(search_methods)\n",
    "pbar = tqdm.tqdm(total=num_files, smoothing=0)\n",
    "for model in models:\n",
    "    for t in transformations:\n",
    "        for cl in constraint_levels:\n",
    "            all_successful = set()\n",
    "            for sm in search_methods:\n",
    "                csv_path = f\"{RESULT_ROOT_DIR}/{model}/{t}/{cl}/{sm}.csv\"\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df = df[df['result_type']==\"Successful\"]\n",
    "                df[\"original_text\"] = df.apply(lambda row: row[\"original_text\"].replace(\"[\",\"\").replace(\"]\",\"\"), axis=1)\n",
    "                if len(all_successful) == 0:\n",
    "                    all_successful = set(df[\"original_text\"])\n",
    "                else:\n",
    "                    all_successful = all_successful.intersection(set(df[\"original_text\"]))\n",
    "                pbar.update(1)\n",
    "            all_successful_attacks.append(all_successful)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = [\"& \\\\multirow{7}{*}{MR} & Greedy (b=1) &\",\n",
    "\"& & Beam Search (b=4) &\", \n",
    "\"& & Beam Search (b=8) &\", \n",
    "\"& & \\\\importanceRankingNameAbbrev (\\\\texttt{UNK}) &\",\n",
    "\"& & \\\\importanceRankingNameAbbrev (\\\\texttt{DEL}) &\",\n",
    "\"& & \\\\importanceRankingNameAbbrev (\\\\texttt{PWWS}) &\",\n",
    "\"& & \\\\importanceRankingNameAbbrev (\\\\texttt{Gradient}) &\", \n",
    "\"& & \\\\importanceRankingNameAbbrev (\\\\texttt{RAND}) &\",\n",
    "\"& & Genetic Algorithm &\",\n",
    " \"& & PSO &\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6887a8adb09c451ba993ab5b9181de39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=90.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "bert-yelp-test/word-swap-embedding/strict\n",
      "---------------------------------------------\n",
      "=============================================\n",
      "bert-yelp-test/word-swap-hownet/strict\n",
      "---------------------------------------------\n",
      "=============================================\n",
      "bert-yelp-test/word-swap-wordnet/strict\n",
      "---------------------------------------------\n",
      "& \\multirow{7}{*}{MR} & Greedy (b=1) &3.41 & 0.948 & 21.5 & 2.52 & 0.945 & 22.8 & 4.76 & 0.943 & 49.9\\\\\n",
      "& & Beam Search (b=4) &3.26 & 0.949 & 20.7 & 2.45 & 0.946 & 22.0 & 4.49 & 0.944 & 46.7\\\\\n",
      "& & Beam Search (b=8) &3.2 & 0.95 & 20.1 & 2.42 & 0.947 & 21.4 & 4.46 & 0.945 & 46.4\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{UNK}) &6.48 & 0.93 & 43.5 & 4.73 & 0.922 & 42.3 & 9.02 & 0.924 & 92.1\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{DEL}) &6.85 & 0.928 & 47.2 & 5.1 & 0.919 & 46.4 & 9.38 & 0.923 & 98.8\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{PWWS}) &4.36 & 0.942 & 27.3 & 3.11 & 0.94 & 28.1 & 6.1 & 0.937 & 66.1\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{Gradient}) &6.16 & 0.933 & 37.8 & 5.58 & 0.913 & 44.5 & 9.1 & 0.925 & 86.4\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{RAND}) &8.18 & 0.92 & 59.1 & 7.46 & 0.898 & 74.6 & 11.16 & 0.914 & 124.8\\\\\n",
      "& & Genetic Algorithm &5.06 & 0.936 & 33.9 & 4.21 & 0.928 & 42.7 & 6.7 & 0.932 & 77.3\\\\\n",
      "& & PSO &6.61 & 0.929 & 47.3 & 6.08 & 0.913 & 62.3 & 9.67 & 0.922 & 111.0\\\\\n",
      "=============================================\n",
      "bert-mr-test/word-swap-embedding/strict\n",
      "---------------------------------------------\n",
      "=============================================\n",
      "bert-mr-test/word-swap-hownet/strict\n",
      "---------------------------------------------\n",
      "=============================================\n",
      "bert-mr-test/word-swap-wordnet/strict\n",
      "---------------------------------------------\n",
      "& \\multirow{7}{*}{MR} & Greedy (b=1) &7.25 & 0.9 & 31.8 & 6.14 & 0.887 & 36.5 & 10.26 & 0.864 & 102.8\\\\\n",
      "& & Beam Search (b=4) &7.22 & 0.901 & 31.4 & 6.1 & 0.887 & 36.1 & 10.1 & 0.866 & 97.9\\\\\n",
      "& & Beam Search (b=8) &7.22 & 0.901 & 31.4 & 6.1 & 0.887 & 36.1 & 10.05 & 0.866 & 101.6\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{UNK}) &9.42 & 0.884 & 42.3 & 7.77 & 0.866 & 48.0 & 14.14 & 0.845 & 141.2\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{DEL}) &9.62 & 0.882 & 46.4 & 7.69 & 0.865 & 46.1 & 14.6 & 0.84 & 146.4\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{PWWS}) &7.36 & 0.898 & 33.8 & 6.22 & 0.884 & 37.6 & 10.8 & 0.865 & 111.1\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{Gradient}) &8.61 & 0.892 & 38.1 & 8.25 & 0.862 & 40.8 & 14.58 & 0.844 & 123.2\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{RAND}) &10.1 & 0.881 & 51.4 & 9.93 & 0.846 & 69.5 & 17.28 & 0.827 & 149.4\\\\\n",
      "& & Genetic Algorithm &8.18 & 0.895 & 35.8 & 6.41 & 0.885 & 37.8 & 12.3 & 0.854 & 124.5\\\\\n",
      "& & PSO &8.71 & 0.894 & 39.0 & 6.46 & 0.884 & 38.7 & 16.08 & 0.839 & 187.8\\\\\n",
      "=============================================\n",
      "bert-snli-test/word-swap-embedding/strict\n",
      "---------------------------------------------\n",
      "=============================================\n",
      "bert-snli-test/word-swap-hownet/strict\n",
      "---------------------------------------------\n",
      "=============================================\n",
      "bert-snli-test/word-swap-wordnet/strict\n",
      "---------------------------------------------\n",
      "& \\multirow{7}{*}{MR} & Greedy (b=1) &5.59 & 0.915 & 37.8 & 5.02 & 0.889 & 31.7 & 6.53 & 0.903 & 55.9\\\\\n",
      "& & Beam Search (b=4) &5.59 & 0.916 & 37.8 & 5.02 & 0.889 & 31.6 & 6.5 & 0.903 & 55.7\\\\\n",
      "& & Beam Search (b=8) &5.59 & 0.916 & 37.8 & 5.02 & 0.889 & 31.6 & 6.5 & 0.903 & 55.9\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{UNK}) &6.56 & 0.911 & 42.8 & 5.65 & 0.887 & 33.4 & 8.03 & 0.899 & 65.5\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{DEL}) &6.77 & 0.91 & 44.0 & 5.81 & 0.887 & 34.2 & 8.22 & 0.898 & 67.6\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{PWWS}) &5.63 & 0.915 & 37.8 & 5.05 & 0.89 & 30.5 & 6.59 & 0.906 & 54.5\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{Gradient}) &6.57 & 0.911 & 41.6 & 5.9 & 0.881 & 37.7 & 8.06 & 0.899 & 65.1\\\\\n",
      "& & \\importanceRankingNameAbbrev (\\texttt{RAND}) &7.06 & 0.909 & 47.7 & 6.19 & 0.884 & 42.9 & 8.65 & 0.895 & 74.6\\\\\n",
      "& & Genetic Algorithm &5.71 & 0.915 & 38.5 & 5.14 & 0.888 & 32.7 & 6.73 & 0.902 & 58.3\\\\\n",
      "& & PSO &5.76 & 0.915 & 38.6 & 5.14 & 0.888 & 32.5 & 6.94 & 0.902 & 58.5\\\\\n"
     ]
    }
   ],
   "source": [
    "num_files = len(models) * len(transformations) * len(constraint_levels) * len(search_methods)\n",
    "pbar = tqdm.tqdm(total=num_files, smoothing=0)\n",
    "i = 0\n",
    "for model in models:\n",
    "    result = [[] for _ in search_methods]\n",
    "    for t in transformations:\n",
    "        for cl in constraint_levels:\n",
    "            print(\"=\"*45)\n",
    "            print(f\"{model}/{t}/{cl}\")\n",
    "            print(\"-\"*45)\n",
    "            k = 0\n",
    "            for sm in search_methods:\n",
    "                csv_path = f\"{RESULT_ROOT_DIR}/{model}/{t}/{cl}/{sm}.csv\"\n",
    "                all_successful = all_successful_attacks[i]\n",
    "                avg_sim, avg_pp_diff, words_changed_percent = evaluator.evaluate(csv_path, all_successful)\n",
    "                result[k].append(f\"{round(words_changed_percent, 2)} & {round(avg_sim, 3)} & {str(round(avg_pp_diff * 100, 1))}\")\n",
    "                pbar.update(1)\n",
    "                k += 1\n",
    "            i+=1\n",
    "    for j, row in enumerate(result):\n",
    "        print(HEADERS[j] + \" & \".join(row) + \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
